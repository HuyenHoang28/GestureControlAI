{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B√°o c√°o Project\n",
    "L·ªõp TTNT-162279, Nh√≥m G15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Th√¥ng tin chung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Th√†nh vi√™n\n",
    "- Ho√†ng Th·ªã H√† Huy·ªÅn 20235346\n",
    "- Tr·∫ßn Ng·ªçc To√†n 20235439\n",
    "- ƒê·ªó Quang Th·ªãnh 20235434\n",
    "- Nguy·ªÖn Minh Tu·∫•n 20232356"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L·ªãch th·ª±c hi·ªán\n",
    "- W02: ƒêƒÉng k√Ω nh√≥m \n",
    "- W03: ƒê·ªÅ xu·∫•t project (28/9)\n",
    "- W08: B√°o c√°o ti·∫øn ƒë·ªô gi·ªØa k·ª≥ (1/11)\n",
    "- W15: Ho√†n th√†nh v√† g·ª≠i b√°o c√°o project (20/12)\n",
    "- W16-18: Tr√¨nh b√†y project, Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ƒê·ªÅ xu·∫•t project (W3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B√†i to√°n\n",
    "Nh√≥m 15 ƒë·ªÅ xu·∫•t m·ªôt ph·∫ßn m·ªÅm ·ª©ng d·ª•ng tr√≠ tu·ªá nh√¢n t·∫°o AI c√≥ kh·∫£ nƒÉng nh·∫≠n di·ªán s√°u c·ª≠ ch·ªâ c·ªßa b√†n tay con ng∆∞·ªùi ƒë·ªÉ ƒëi·ªÅu khi·ªÉn v√† th·ª±c hi·ªán c√°c thao t√°c tr√™n thi·∫øt b·ªã th√¥ng minh m√† kh√¥ng c·∫ßn ph·∫£i tr·ª±c ti·∫øp ch·∫°m v√†o m√†n h√¨nh c·∫£m ·ª©ng. Gi·∫£i ph√°p n√†y kh√¥ng ch·ªâ g√≥p ph·∫ßn m·ªü ra m·ªôt tr·∫£i nghi·ªám m·ªõi m·∫ª v√† hi·ªán ƒë·∫°i m√† c√≤n h∆∞·ªõng ƒë·∫øn vi·ªác h·ªó tr·ª£ ng∆∞·ªùi d√πng trong nhi·ªÅu t√¨nh hu·ªëng khi m√† vi·ªác tr·ª±c ti·∫øp ti·∫øp x√∫c v·ªõi m√†n h√¨nh ƒëi·ªán tho·∫°i tr·ªü n√™n kh√≥ khƒÉn. Sau khi ho√†n th√†nh d·ª± √°n, nh√≥m mong mu·ªën ƒë·∫°t ƒë∆∞·ª£c c√°c k·∫øt qu·∫£ sau: \n",
    "\n",
    "- Ph·∫ßn m·ªÅm Tr√≠ tu·ªá nh√¢n t·∫°o s·ª≠ d·ª•ng tr√™n c√°c laptop c√≥ camera, c√≥ kh·∫£ nƒÉng nh·∫≠n di·ªán ng√≥n tay, c·ª≠ ch·ªâ tay, v√† chuy·ªÉn c√°c c·ª≠ ch·ªâ ƒë√≥ th√†nh th√¥ng tin, c√≥ giao di·ªán ƒë∆°n gi·∫£n, d·ªÖ s·ª≠ d·ª•ng. \n",
    "\n",
    "- M√¥ h√¨nh Tr√≠ tu·ªá nh√¢n t·∫°o ƒë∆∞·ª£c hu·∫•n luy·ªán s·∫Ω c√≥ ƒë·ªô tin c·∫≠y, ch√≠nh x√°c cao (>70-80%), kh·∫£ nƒÉng ph·∫£n h·ªìi nhanh theo th·ªùi gian th·ª±c. \n",
    "\n",
    "- C√≥ kh·∫£ nƒÉng s·ª≠a ƒë·ªïi, b·ªï sung, ph√°t tri·ªÉn th√™m t√≠nh nƒÉng, ph·ª•c v·ª• cho nhi·ªÅu m·ª•c ƒë√≠ch (V√≠ d·ª• nh∆∞ th√†nh vi√™n nh√≥m mu·ªën d√πng l√†m ƒë·ªÅ t√†i NCKH, thi c√°c cu·ªôc thi,...) \n",
    "\n",
    "- N√¢ng cao tr·∫£i nghi·ªám ng∆∞·ªùi d√πng, t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t v√† an to√†n khi √°p d·ª•ng d·ª± √°n v√†o c√°c lƒ©nh v·ª±c nh∆∞ m√¥i tr∆∞·ªùng, y t·∫ø,...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ph∆∞∆°ng ph√°p\n",
    "S·ª≠ d·ª•ng h√¨nh ·∫£nh c·ªßa b√†n tay: ·∫¢nh ƒë∆°n (single images) c·ªßa c√°c c·ª≠ ch·ªâ kh√°c nhau. \n",
    "\n",
    "B√†i to√°n nh·∫≠n di·ªán c·ª≠ ch·ªâ tay ƒë∆∞·ª£c th·ª±c hi·ªán d·ª±a tr√™n h√¨nh ·∫£nh ƒë·∫ßu v√†o l√† ·∫£nh b√†n tay c·ªßa ng∆∞·ªùi d√πng, c√≥ th·ªÉ l√† ·∫£nh ƒë∆°n (single image) ho·∫∑c chu·ªói ·∫£nh (video). M·ª•c ti√™u l√† ph√¢n lo·∫°i c√°c c·ª≠ ch·ªâ kh√°c nhau d·ª±a tr√™n d·ªØ li·ªáu h√¨nh ·∫£nh ho·∫∑c khung x∆∞∆°ng b√†n tay.\n",
    "\n",
    "1. Ki·ªÉu d·ªØ li·ªáu\n",
    "\t‚Ä¢\tRGB images: ·∫¢nh m√†u thu ƒë∆∞·ª£c t·ª´ webcam ho·∫∑c camera ƒëi·ªán tho·∫°i.\n",
    "\t‚Ä¢\tDepth images: ·∫¢nh chi·ªÅu s√¢u t·ª´ c√°c thi·∫øt b·ªã nh∆∞ Kinect ho·∫∑c RealSense.\n",
    "\t‚Ä¢\tSkeleton/Keypoints data: D·ªØ li·ªáu v·ªã tr√≠ c√°c kh·ªõp tay (landmarks).\n",
    "\n",
    "2. C√°c dataset ph·ªï bi·∫øn\n",
    "\t‚Ä¢\tASL Alphabet Dataset: B·ªô d·ªØ li·ªáu c√°c ch·ªØ c√°i trong ng√¥n ng·ªØ k√Ω hi·ªáu M·ªπ.\n",
    "\t‚Ä¢\tHand Gesture Recognition Database (HGR): B·ªô d·ªØ li·ªáu c√°c c·ª≠ ch·ªâ tay th√¥ng th∆∞·ªùng.\n",
    "\t‚Ä¢\tNVGesture: B·ªô d·ªØ li·ªáu video cho c√°c c·ª≠ ch·ªâ t∆∞∆°ng t√°c v·ªõi thi·∫øt b·ªã.\n",
    "\t‚Ä¢\tFingerSpelling Dataset: D·ªØ li·ªáu ng√¥n ng·ªØ k√Ω hi·ªáu theo t·ª´ng ng√≥n tay.\n",
    "\n",
    "3. K·ªπ thu·∫≠t s·ª≠ d·ª•ng\n",
    "\n",
    "a. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (Data Preprocessing)\n",
    "\t‚Ä¢\tChu·∫©n h√≥a k√≠ch th∆∞·ªõc ·∫£nh v√† c∆∞·ªùng ƒë·ªô s√°ng (resize, normalize).\n",
    "\t‚Ä¢\tKh·ª≠ nhi·ªÖu b·∫±ng c√°c b·ªô l·ªçc nh∆∞ Gaussian blur ho·∫∑c bilateral filter.\n",
    "\t‚Ä¢\tT√°ch n·ªÅn ƒë·ªÉ l√†m n·ªïi b·∫≠t v√πng b√†n tay (background subtraction).\n",
    "\t‚Ä¢\tX√°c ƒë·ªãnh v√πng b√†n tay b·∫±ng MediaPipe Hands, OpenCV, ho·∫∑c YOLO.\n",
    "\n",
    "b. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (Feature Extraction)\n",
    "\t‚Ä¢\tCNN feature maps: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng kh√¥ng gian t·ª´ ·∫£nh RGB.\n",
    "\t‚Ä¢\tKNN gi√∫p x√°c ƒë·ªãnh c·ª≠ ch·ªâ tay d·ª±a tr√™n ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o. VD: T·ªça ƒë·ªô kh·ªõp tay (skeleton/keypoints).\n",
    "\t‚Ä¢\tHOG (Histogram of Oriented Gradients): Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng h∆∞·ªõng c·∫°nh cho nh·∫≠n d·∫°ng h√¨nh d·∫°ng.\n",
    "\t‚Ä¢\tLandmark-based descriptors: T√≠nh g√≥c gi·ªØa c√°c ng√≥n tay ho·∫∑c kho·∫£ng c√°ch gi·ªØa c√°c kh·ªõp.\n",
    "\n",
    "c. M√¥ h√¨nh h·ªçc s√¢u (Deep Learning Models)\n",
    "\t‚Ä¢\tCNN (Convolutional Neural Network): M√¥ h√¨nh n·ªÅn t·∫£ng cho nh·∫≠n d·∫°ng h√¨nh ·∫£nh.\n",
    "\t‚Ä¢\tMobileNet / EfficientNet: M√¥ h√¨nh nh·∫π, t·ªëi ∆∞u cho thi·∫øt b·ªã di ƒë·ªông.\n",
    "\t‚Ä¢\tResNet / Inception / VGG: Cho ƒë·ªô ch√≠nh x√°c cao v·ªõi d·ªØ li·ªáu l·ªõn.\n",
    "\t‚Ä¢\t3D CNN ho·∫∑c I3D: D√πng cho d·ªØ li·ªáu video, h·ªçc ƒë·∫∑c tr∆∞ng kh√¥ng gian‚Äìth·ªùi gian.\n",
    "\t‚Ä¢\tLSTM / GRU: M√¥ h√¨nh tu·∫ßn t·ª± cho d·ªØ li·ªáu c·ª≠ ch·ªâ li√™n ti·∫øp.\n",
    "\t‚Ä¢\tVision Transformer (ViT) ho·∫∑c TimeSformer: M√¥ h√¨nh hi·ªán ƒë·∫°i s·ª≠ d·ª•ng c∆° ch·∫ø t·ª± ch√∫ √Ω (attention).\n",
    "\n",
    "d. Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh (Training & Evaluation)\n",
    "\t‚Ä¢\t√Åp d·ª•ng Data Augmentation (xoay, l·∫≠t, thay ƒë·ªïi s√°ng, ph√≥ng to/thu nh·ªè) ƒë·ªÉ tƒÉng ƒë·ªô ƒëa d·∫°ng.\n",
    "\t‚Ä¢\tChia d·ªØ li·ªáu th√†nh 3 ph·∫ßn: Train ‚Äì Validation ‚Äì Test (t·ªâ l·ªá th∆∞·ªùng 70‚Äì20‚Äì10).\n",
    "\t‚Ä¢\tƒê√°nh gi√° m√¥ h√¨nh b·∫±ng c√°c ch·ªâ s·ªë: Accuracy, Precision, Recall, F1-score, Confusion Matrix.\n",
    "\n",
    "e. Tri·ªÉn khai m√¥ h√¨nh (Deployment)\n",
    "\t‚Ä¢\t·ª®ng d·ª•ng nh·∫≠n di·ªán c·ª≠ ch·ªâ real-time qua webcam, ho·∫∑c t√≠ch h·ª£p v√†o ·ª©ng d·ª•ng di ƒë·ªông.\n",
    "\t‚Ä¢\tC√°c framework th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng: TensorFlow Lite, ONNX, OpenCV, MediaPipe, PyTorch Mobile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ph√¢n c√¥ng\n",
    "- Ho√†ng Th·ªã H√† Huy·ªÅn: \n",
    "\n",
    "Thu th·∫≠p, x·ª≠ l√Ω d·ªØ li·ªáu: T√°ch n·ªÅn ·∫£nh m·∫´u, chu·∫©n h√≥a, g√°n nh√£n d·ªØ li·ªáu ƒë·ªÉ ph·ª•c v·ª• hu·∫•n luy·ªán m√¥ h√¨nh. \n",
    "\n",
    "- ƒê·ªó Quang Th·ªãnh: \n",
    "\n",
    "T√¨m hi·ªÉu l√Ω thuy·∫øt n·ªÅn t·∫£ng, ƒë·ªÅ xu·∫•t c√°c m√¥ h√¨nh ph√π h·ª£p v√† thi·∫øt k·∫ø thu·∫≠t to√°n. \n",
    "\n",
    "- Tr·∫ßn Ng·ªçc To√†n: \n",
    "\n",
    "T·ªëi ∆∞u v√† chi ti·∫øt h√≥a thu·∫≠t to√°n ch√≠nh m√† c·∫£ nh√≥m th·ªëng nh·∫•t s·ª≠ d·ª•ng, hu·∫•n luy·ªán m√¥ h√¨nh. \n",
    "\n",
    "- Nguy·ªÖn Minh Tu·∫•n: \n",
    "\n",
    "Tri·ªÉn khai, ki·ªÉm th·ª≠, t√≠nh to√°n ƒë·ªô ch√≠nh x√°c, x·ª≠ l√Ω d·ªØ li·ªáu ƒë·ªÉ ƒë∆∞a v√†o b√†i b√°o c√°o. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ti·∫øn ƒë·ªô gi·ªØa k·ª≥ (W8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ch∆∞∆°ng tr√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "DATASET_PATH = \"landmarks.npy\"     \n",
    "MODEL_PATH   = \"knn_model.joblib\"\n",
    "SCALER_PATH  = \"scaler.joblib\"\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n",
    "\n",
    "def extract_hand_landmarks(results):\n",
    "    \"\"\"Tr·∫£ v·ªÅ feature vector 126 chi·ªÅu, lu√¥n g·ªìm 2 tay\"\"\"\n",
    "    num_landmarks = 21\n",
    "    all_coords = []\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand in results.multi_hand_landmarks:\n",
    "            coords = np.array([[lm.x, lm.y, lm.z] for lm in hand.landmark])\n",
    "            base = coords[0].copy()\n",
    "            coords -= base\n",
    "            max_dist = np.max(np.linalg.norm(coords, axis=1))\n",
    "            if max_dist > 1e-6:\n",
    "                coords /= max_dist\n",
    "            all_coords.append(coords.flatten())\n",
    "\n",
    "    # ƒê·∫£m b·∫£o lu√¥n 2 tay\n",
    "    while len(all_coords) < 2:\n",
    "        all_coords.append(np.zeros(num_landmarks*3))\n",
    "\n",
    "    return np.concatenate(all_coords) \n",
    "\n",
    "def load_dataset():\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        return list(np.load(DATASET_PATH, allow_pickle=True))\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def save_dataset(dataset):\n",
    "    np.save(DATASET_PATH, np.array(dataset, dtype=object))\n",
    "    print(f\"‚úÖ Saved dataset: {DATASET_PATH} (samples={len(dataset)})\")\n",
    "\n",
    "def collect(label):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    dataset = load_dataset()\n",
    "    print(\"üì∑ Camera opened. Press SPACE to capture a sample for label:\", label)\n",
    "    print(\"Press q to quit.\")\n",
    "    with mp_hands.Hands(static_image_mode=False,\n",
    "                        max_num_hands=2,\n",
    "                        min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5) as hands:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Cannot read camera frame.\")\n",
    "                break\n",
    "            img = cv2.flip(frame, 1)\n",
    "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(rgb)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                    mp_drawing.draw_landmarks(img, hand_landmarks, mp_hands_connections)\n",
    "                    label_h = handedness.classification[0].label  # Left / Right\n",
    "                    coords = hand_landmarks.landmark[0]\n",
    "                    h, w, _ = img.shape\n",
    "                    x, y = int(coords.x * w), int(coords.y * h)\n",
    "                    cv2.putText(img, label_h, (x - 20, y - 20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "            cv2.putText(img, f\"Label: {label}\", (10,30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.putText(img, \"Space: save sample  |  q: quit\", (10,60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 1)\n",
    "            cv2.imshow(\"Collect gestures\", img)\n",
    "\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            if key == 32:  # space\n",
    "                landmark_vec = extract_hand_landmarks(results)\n",
    "                if landmark_vec is not None:\n",
    "                    dataset.append((label, landmark_vec))\n",
    "                    print(f\"Saved sample #{len(dataset)} for label '{label}'\")\n",
    "                    save_dataset(dataset)\n",
    "                else:\n",
    "                    print(\"No hand detected. Try again.\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def train():\n",
    "    dataset = load_dataset()\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No data found. Use collect mode to gather samples first.\")\n",
    "        return\n",
    "    labels = [d[0] for d in dataset]\n",
    "    X = np.stack([d[1] for d in dataset], axis=0)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    unique_labels = sorted(list(set(labels)))\n",
    "    label2idx = {lab:i for i,lab in enumerate(unique_labels)}\n",
    "    y_idx = np.array([label2idx[l] for l in y])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_scaled, y_idx, test_size=0.2, random_state=42, stratify=y_idx)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(Xtr, ytr)\n",
    "    acc = knn.score(Xte, yte)\n",
    "    print(f\"Trained KNN. Test accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "    joblib.dump({\n",
    "        \"model\": knn,\n",
    "        \"label2idx\": label2idx,\n",
    "        \"idx2label\": {v:k for k,v in label2idx.items()}\n",
    "    }, MODEL_PATH)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    print(f\"Saved model to {MODEL_PATH} and scaler to {SCALER_PATH}\")\n",
    "\n",
    "def predict():\n",
    "    if not os.path.exists(MODEL_PATH) or not os.path.exists(SCALER_PATH):\n",
    "        print(\"Model or scaler not found. Train first.\")\n",
    "        return\n",
    "\n",
    "    data = joblib.load(MODEL_PATH)\n",
    "    knn = data[\"model\"]\n",
    "    idx2label = data[\"idx2label\"]\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_hands.Hands(static_image_mode=False,\n",
    "                        max_num_hands=2,\n",
    "                        min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5) as hands:\n",
    "        print(\"Starting real-time prediction. Press q to quit.\")\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            img = cv2.flip(frame, 1)\n",
    "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(rgb)\n",
    "\n",
    "            label_text = \"No hand\"\n",
    "            prob_text = \"\"\n",
    "            if results.multi_hand_landmarks:\n",
    "                # Hand landmarks + Hand side\n",
    "                for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                    mp_drawing.draw_landmarks(img, hand_landmarks, mp_hands_connections)\n",
    "                    label_h = handedness.classification[0].label  # Left / Right\n",
    "                    coords = hand_landmarks.landmark[0]\n",
    "                    h, w, _ = img.shape\n",
    "                    x, y = int(coords.x * w), int(coords.y * h)\n",
    "                    cv2.putText(img, label_h, (x - 20, y - 20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "                vec = extract_hand_landmarks(results)\n",
    "                vec_scaled = scaler.transform(vec.reshape(1, -1))\n",
    "                pred_idx = knn.predict(vec_scaled)[0]\n",
    "                if hasattr(knn, \"predict_proba\"):\n",
    "                    probs = knn.predict_proba(vec_scaled)[0]\n",
    "                    conf = probs[pred_idx]\n",
    "                    prob_text = f\"{conf*100:.1f}%\"\n",
    "                label_text = idx2label[int(pred_idx)]\n",
    "\n",
    "            cv2.putText(img, f\"Gesture: {label_text}\", (10,30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "            if prob_text:\n",
    "                cv2.putText(img, f\"Conf: {prob_text}\", (10,65),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,200,255), 2)\n",
    "\n",
    "            cv2.imshow(\"Hand Gesture Recognition\", img)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206fcaa742ef466abc3899d7c5d51abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Ch·ªçn label:', options=(('1_fist (0 samples)', '1_fist'), ('2_fist (0 samples)', '2_fist'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6d23d1ef844b63922817aad8be69cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='B·∫Øt ƒë·∫ßu collect', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === COLLECT DATA MODE ===\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "DATASET_PATH = \"landmarks.npy\"\n",
    "\n",
    "# Lable list\n",
    "labels = ['1_fist', '2_fist', '1_palm', '2_palm', '1_thumbs_up', '1_thumbs_down', '1_ok', '1_peace']\n",
    "\n",
    "def get_sample_count(label):\n",
    "    #Count sample of each label.\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        return 0\n",
    "    data = np.load(DATASET_PATH, allow_pickle=True)\n",
    "    return sum(1 for item in data if item[0] == label)\n",
    "\n",
    "def make_options():\n",
    "    #Count samples of each label in the list.\n",
    "    return [(f\"{label} ({get_sample_count(label)} samples)\", label) for label in labels]\n",
    "\n",
    "# drop-down with labels + num of samples\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=make_options(),\n",
    "    value=labels[0],\n",
    "    description='Choose label:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Collect button\n",
    "button = widgets.Button(description=\"Collect\")\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    print(f\"Collect for label: {dropdown.value}\")\n",
    "    collect(dropdown.value)  \n",
    "    print(f\"Finish collecting label: {dropdown.value}\")\n",
    "\n",
    "    # Update num of samples\n",
    "    dropdown.options = make_options()\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "display(dropdown, button)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained KNN. Test accuracy: 100.00%\n",
      "Saved model to knn_model.joblib and scaler to scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "# === TRAIN MODE ===\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time prediction. Press q to quit.\n"
     ]
    }
   ],
   "source": [
    "# === PREDICT MODE ===\n",
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ph√¢n t√≠ch k·∫øt qu·∫£, v·∫•n ƒë·ªÅ g·∫∑p ph·∫£i\n",
    "1. Ph√¢n t√≠ch k·∫øt qu·∫£\n",
    " - Nh√≥m ƒë√£ t·∫°o ra m√¥ h√¨nh Tr√≠ tu·ªá nh√¢n t·∫°o c√≥ kh·∫£ nƒÉng nh·∫≠n di·ªán c√°c ng√≥n tay v√† c·ª≠ ch·ªâ, c√≥ ƒë·ªô ch√≠nh x√°c cao.\n",
    " - M√¥ h√¨nh Tr√≠ tu·ªá nh√¢n t·∫°o s·ª≠ d·ª•ng m√¥ h√¨nh hu·∫•n luy·ªán K-Nearest Neighbors (KNN), sau qu√° tr√¨nh hu·∫•n luy·ªán ƒë√£ c√≥ kh·∫£ nƒÉng ph√¢n bi·ªát c√°c c·ª≠ ch·ªâ tay c∆° b·∫£n, v·ªõi ƒë·ªô ch√≠nh x√°c kh√° cao khi c√°c c·ª≠ ch·ªâ c√≥ s·ª± kh√°c nhau nhi·ªÅu.\n",
    " - Cho ƒë·∫øn hi·ªán t·∫°i, m√¥ h√¨nh v·∫´n ƒëang ho·∫°t ƒë·ªông kh√° t·ªët, v√† ch∆∞a c√≥ l·ªói nghi√™m tr·ªçng.\n",
    "2. V·∫•n ƒë·ªÅ g·∫∑p ph·∫£i\n",
    " - C√°c th√†nh vi√™n nh√≥m ƒë·ªÅu ch∆∞a c√≥ kinh nghi·ªám trong vi·ªác l√†m c√°c project v·ªÅ AI tr∆∞·ªõc ƒë√¢y, n√™n l√∫c th·ª±c hi·ªán m·ªü r·ªông project v·∫´n c√≤n nhi·ªÅu kh√≥ khƒÉn, v∆∞·ªõng m·∫Øc. Tuy nhi√™n, nh·ªù s·ª± ph·ªï bi·∫øn c·ªßa c√°c k√™nh th√¥ng tin, cho ƒë·∫øn hi·ªán t·∫°i project v·∫´n ch∆∞a g·∫∑p tr·ª•c tr·∫∑c g√¨ l·ªõn.\n",
    " - C√°c th√†nh vi√™n ch∆∞a th·ªëng nh·∫•t ƒë∆∞·ª£c vi·ªác ·ª©ng d·ª•ng m√¥ h√¨nh tr√™n v√†o v·∫•n ƒë·ªÅ th·ª±c t·∫ø n√†o, hi·ªán nay, nh√≥m ƒëang c√≥ 2 h∆∞·ªõng: √Åp d·ª•ng v√†o vi·ªác nh·∫≠n bi·∫øt c·ª≠ ch·ªâ tay ƒë·ªÉ chuy·ªÉn th√†nh ch·ªØ t∆∞∆°ng ·ª©ng (theo chu·∫©n ASL ho·∫∑c VSL), ho·∫∑c ·ª®ng d·ª•ng v√†o vi·ªác s·ª≠ d·ª•ng c·ª≠ ch·ªâ tay ƒë·ªÉ ƒëi·ªÅu khi·ªÉn m√°y t√≠nh ·ªü kho·∫£ng c√°ch v·ª´a (2-3 m)\n",
    " - V·ªÅ ti·∫øn ƒë·ªô th·ª±c hi·ªán, nh√≥m s·∫Ω c·ªë g·∫Øng th·ªëng nh·∫•t vi·ªác ·ª©ng d·ª•ng m√¥ h√¨nh v√†o ƒë√¢u trong tu·∫ßn t·ªõi, ƒë·ªÉ nhanh ch√≥ng ho√†n thi·ªán c√°c b∆∞·ªõc ti·∫øp theo nh∆∞ ƒë√°nh gi√°, ki·ªÉm th·ª≠, ƒë∆∞a ra b·∫£n ch√≠nh th·ª©c c·ªßa project ƒë·ªÉ vi·∫øt b√°o c√°o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. C·∫≠p nh·∫≠t k·∫øt qu·∫£ cu·ªëi k·ª≥ (W15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi ti·∫øt ph∆∞∆°ng ph√°p, d·ªØ li·ªáu \n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ch∆∞∆°ng tr√¨nh\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ph√¢n t√≠ch, ƒë√°nh gi√° k·∫øt qu·∫£\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C·∫≠p nh·∫≠t ph√¢n c√¥ng, kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác\n",
    "<!-- c√¥ng vi·ªác c·ªßa c√°c th√†nh vi√™n, t·ª∑ l·ªá ƒë√≥ng g√≥p c·ªßa c√°c th√†nh vi√™n -->\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
