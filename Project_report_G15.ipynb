{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Báo cáo Project\n",
    "Lớp TTNT-162279, Nhóm G15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Thông tin chung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thành viên\n",
    "- Hoàng Thị Hà Huyền 20235346\n",
    "- Trần Ngọc Toàn 20235439\n",
    "- Đỗ Quang Thịnh 20235434\n",
    "- Nguyễn Minh Tuấn 20232356"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lịch thực hiện\n",
    "- W02: Đăng ký nhóm \n",
    "- W03: Đề xuất project (28/9)\n",
    "- W08: Báo cáo tiến độ giữa kỳ (1/11)\n",
    "- W15: Hoàn thành và gửi báo cáo project (20/12)\n",
    "- W16-18: Trình bày project, Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Đề xuất project (W3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài toán\n",
    "Nhóm 15 đề xuất một phần mềm ứng dụng trí tuệ nhân tạo AI có khả năng nhận diện sáu cử chỉ của bàn tay con người để điều khiển và thực hiện các thao tác trên thiết bị thông minh mà không cần phải trực tiếp chạm vào màn hình cảm ứng. Giải pháp này không chỉ góp phần mở ra một trải nghiệm mới mẻ và hiện đại mà còn hướng đến việc hỗ trợ người dùng trong nhiều tình huống khi mà việc trực tiếp tiếp xúc với màn hình điện thoại trở nên khó khăn. Sau khi hoàn thành dự án, nhóm mong muốn đạt được các kết quả sau: \n",
    "\n",
    "- Phần mềm Trí tuệ nhân tạo sử dụng trên các laptop có camera, có khả năng nhận diện ngón tay, cử chỉ tay, và chuyển các cử chỉ đó thành thông tin, có giao diện đơn giản, dễ sử dụng. \n",
    "\n",
    "- Mô hình Trí tuệ nhân tạo được huấn luyện sẽ có độ tin cậy, chính xác cao (>70-80%), khả năng phản hồi nhanh theo thời gian thực. \n",
    "\n",
    "- Có khả năng sửa đổi, bổ sung, phát triển thêm tính năng, phục vụ cho nhiều mục đích (Ví dụ như thành viên nhóm muốn dùng làm đề tài NCKH, thi các cuộc thi,...) \n",
    "\n",
    "- Nâng cao trải nghiệm người dùng, tối ưu hóa hiệu suất và an toàn khi áp dụng dự án vào các lĩnh vực như môi trường, y tế,...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phương pháp\n",
    "Sử dụng hình ảnh của bàn tay: Ảnh đơn (single images) của các cử chỉ khác nhau. \n",
    "\n",
    "Bài toán nhận diện cử chỉ tay được thực hiện dựa trên hình ảnh đầu vào là ảnh bàn tay của người dùng, có thể là ảnh đơn (single image) hoặc chuỗi ảnh (video). Mục tiêu là phân loại các cử chỉ khác nhau dựa trên dữ liệu hình ảnh hoặc khung xương bàn tay.\n",
    "\n",
    "1. Kiểu dữ liệu\n",
    "\t•\tRGB images: Ảnh màu thu được từ webcam hoặc camera điện thoại.\n",
    "\t•\tDepth images: Ảnh chiều sâu từ các thiết bị như Kinect hoặc RealSense.\n",
    "\t•\tSkeleton/Keypoints data: Dữ liệu vị trí các khớp tay (landmarks).\n",
    "\n",
    "2. Các dataset phổ biến\n",
    "\t•\tASL Alphabet Dataset: Bộ dữ liệu các chữ cái trong ngôn ngữ ký hiệu Mỹ.\n",
    "\t•\tHand Gesture Recognition Database (HGR): Bộ dữ liệu các cử chỉ tay thông thường.\n",
    "\t•\tNVGesture: Bộ dữ liệu video cho các cử chỉ tương tác với thiết bị.\n",
    "\t•\tFingerSpelling Dataset: Dữ liệu ngôn ngữ ký hiệu theo từng ngón tay.\n",
    "\n",
    "3. Kỹ thuật sử dụng\n",
    "\n",
    "a. Tiền xử lý dữ liệu (Data Preprocessing)\n",
    "\t•\tChuẩn hóa kích thước ảnh và cường độ sáng (resize, normalize).\n",
    "\t•\tKhử nhiễu bằng các bộ lọc như Gaussian blur hoặc bilateral filter.\n",
    "\t•\tTách nền để làm nổi bật vùng bàn tay (background subtraction).\n",
    "\t•\tXác định vùng bàn tay bằng MediaPipe Hands, OpenCV, hoặc YOLO.\n",
    "\n",
    "b. Trích xuất đặc trưng (Feature Extraction)\n",
    "\t•\tCNN feature maps: Trích xuất đặc trưng không gian từ ảnh RGB.\n",
    "\t•\tKNN giúp xác định cử chỉ tay dựa trên đặc trưng đầu vào. VD: Tọa độ khớp tay (skeleton/keypoints).\n",
    "\t•\tHOG (Histogram of Oriented Gradients): Trích xuất đặc trưng hướng cạnh cho nhận dạng hình dạng.\n",
    "\t•\tLandmark-based descriptors: Tính góc giữa các ngón tay hoặc khoảng cách giữa các khớp.\n",
    "\n",
    "c. Mô hình học sâu (Deep Learning Models)\n",
    "\t•\tCNN (Convolutional Neural Network): Mô hình nền tảng cho nhận dạng hình ảnh.\n",
    "\t•\tMobileNet / EfficientNet: Mô hình nhẹ, tối ưu cho thiết bị di động.\n",
    "\t•\tResNet / Inception / VGG: Cho độ chính xác cao với dữ liệu lớn.\n",
    "\t•\t3D CNN hoặc I3D: Dùng cho dữ liệu video, học đặc trưng không gian–thời gian.\n",
    "\t•\tLSTM / GRU: Mô hình tuần tự cho dữ liệu cử chỉ liên tiếp.\n",
    "\t•\tVision Transformer (ViT) hoặc TimeSformer: Mô hình hiện đại sử dụng cơ chế tự chú ý (attention).\n",
    "\n",
    "d. Huấn luyện và đánh giá mô hình (Training & Evaluation)\n",
    "\t•\tÁp dụng Data Augmentation (xoay, lật, thay đổi sáng, phóng to/thu nhỏ) để tăng độ đa dạng.\n",
    "\t•\tChia dữ liệu thành 3 phần: Train – Validation – Test (tỉ lệ thường 70–20–10).\n",
    "\t•\tĐánh giá mô hình bằng các chỉ số: Accuracy, Precision, Recall, F1-score, Confusion Matrix.\n",
    "\n",
    "e. Triển khai mô hình (Deployment)\n",
    "\t•\tỨng dụng nhận diện cử chỉ real-time qua webcam, hoặc tích hợp vào ứng dụng di động.\n",
    "\t•\tCác framework thường được sử dụng: TensorFlow Lite, ONNX, OpenCV, MediaPipe, PyTorch Mobile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phân công\n",
    "- Hoàng Thị Hà Huyền: \n",
    "\n",
    "Thu thập, xử lý dữ liệu: Tách nền ảnh mẫu, chuẩn hóa, gán nhãn dữ liệu để phục vụ huấn luyện mô hình. \n",
    "\n",
    "- Đỗ Quang Thịnh: \n",
    "\n",
    "Tìm hiểu lý thuyết nền tảng, đề xuất các mô hình phù hợp và thiết kế thuật toán. \n",
    "\n",
    "- Trần Ngọc Toàn: \n",
    "\n",
    "Tối ưu và chi tiết hóa thuật toán chính mà cả nhóm thống nhất sử dụng, huấn luyện mô hình. \n",
    "\n",
    "- Nguyễn Minh Tuấn: \n",
    "\n",
    "Triển khai, kiểm thử, tính toán độ chính xác, xử lý dữ liệu để đưa vào bài báo cáo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tiến độ giữa kỳ (W8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chương trình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DATASET_PATH = \"landmarks.npy\"     \n",
    "MODEL_PATH   = \"knn_model.joblib\"\n",
    "SCALER_PATH  = \"scaler.joblib\"\n",
    "\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n",
    "\n",
    "def extract_hand_landmarks(results):\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return None\n",
    "\n",
    "    hand_list = results.multi_hand_landmarks[:2] #limit 2 hands only\n",
    "    \n",
    "    all_coords = []\n",
    "    for hand in hand_list:\n",
    "        coords = np.array([[lm.x, lm.y, lm.z] for lm in hand.landmark]) #extract (x,y,z) cords of 21 point in each hand\n",
    "        base = coords[0].copy()\n",
    "        coords -= base\n",
    "        max_dist = np.max(np.linalg.norm(coords, axis=1))\n",
    "        if max_dist > 1e-6:\n",
    "            coords /= max_dist\n",
    "        all_coords.append(coords.flatten())\n",
    "\n",
    "    # 1 hand -> other hand has (0,...)\n",
    "    if len(all_coords) == 1:\n",
    "        all_coords.append(np.zeros_like(all_coords[0]))\n",
    "\n",
    "    feature = np.concatenate(all_coords)\n",
    "    return feature  \n",
    "\n",
    "def load_dataset():\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        return list(np.load(DATASET_PATH, allow_pickle=True))\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def save_dataset(dataset):\n",
    "    np.save(DATASET_PATH, np.array(dataset, dtype=object))\n",
    "    print(f\"Saved dataset: {DATASET_PATH} (samples={len(dataset)})\")\n",
    "\n",
    "def collect(label):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    dataset = load_dataset()\n",
    "    print(\"Camera opened. Press SPACE to capture a sample for label:\", label)\n",
    "    print(\"Press q to quit.\")\n",
    "    with mp_hands.Hands(static_image_mode=False,\n",
    "                        max_num_hands=2,\n",
    "                        min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5) as hands:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Cannot read camera frame.\")\n",
    "                break\n",
    "            img = cv2.flip(frame, 1)\n",
    "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(rgb)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                    mp_drawing.draw_landmarks(img, hand_landmarks, mp_hands_connections)\n",
    "                    label_h = handedness.classification[0].label  # Left / Right\n",
    "                    coords = hand_landmarks.landmark[0]\n",
    "                    h, w, _ = img.shape\n",
    "                    x, y = int(coords.x * w), int(coords.y * h)\n",
    "                    cv2.putText(img, label_h, (x - 20, y - 20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "            cv2.putText(img, f\"Label: {label}\", (10,30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.putText(img, \"Space: save sample  |  q: quit\", (10,60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 1)\n",
    "            cv2.imshow(\"Collect gestures\", img)\n",
    "\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            if key == 32:  # space\n",
    "                landmark_vec = extract_hand_landmarks(results)\n",
    "                if landmark_vec is not None:\n",
    "                    dataset.append((label, landmark_vec))\n",
    "                    print(f\"Saved sample #{len(dataset)} for label '{label}'\")\n",
    "                    save_dataset(dataset)\n",
    "                else:\n",
    "                    print(\"No hand detected. Try again.\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def train():\n",
    "    dataset = load_dataset()\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No data found. Use collect mode to gather samples first.\")\n",
    "        return\n",
    "    labels = [d[0] for d in dataset]\n",
    "    X = np.stack([d[1] for d in dataset], axis=0)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    unique_labels = sorted(list(set(labels)))\n",
    "    label2idx = {lab:i for i,lab in enumerate(unique_labels)}\n",
    "    y_idx = np.array([label2idx[l] for l in y])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_scaled, y_idx, test_size=0.2, random_state=42, stratify=y_idx)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(Xtr, ytr)\n",
    "    acc = knn.score(Xte, yte)\n",
    "    print(f\"Trained KNN. Test accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "    joblib.dump({\n",
    "        \"model\": knn,\n",
    "        \"label2idx\": label2idx,\n",
    "        \"idx2label\": {v:k for k,v in label2idx.items()}\n",
    "    }, MODEL_PATH)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    print(f\"Saved model to {MODEL_PATH} and scaler to {SCALER_PATH}\")\n",
    "\n",
    "def predict():\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"Model not found. Train first.\")\n",
    "        return\n",
    "\n",
    "    data = joblib.load(MODEL_PATH)\n",
    "    knn = data[\"model\"]\n",
    "    idx2label = data[\"idx2label\"]\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_hands.Hands(static_image_mode=False,\n",
    "                        max_num_hands=2,\n",
    "                        min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5) as hands:\n",
    "        print(\"Starting real-time prediction. Press q to quit.\")\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            img = cv2.flip(frame, 1)\n",
    "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(rgb)\n",
    "\n",
    "            label_text = \"No hand\"\n",
    "            prob_text = \"\"\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                    mp_drawing.draw_landmarks(img, hand_landmarks, mp_hands_connections)\n",
    "                    label_h = handedness.classification[0].label\n",
    "                    coords = hand_landmarks.landmark[0]\n",
    "                    h, w, _ = img.shape\n",
    "                    x, y = int(coords.x * w), int(coords.y * h)\n",
    "                    cv2.putText(img, label_h, (x - 20, y - 20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "                vec = extract_hand_landmarks(results)\n",
    "                if vec is not None:\n",
    "                    # Check for bug 3 hands\n",
    "                    if vec.shape[0] != scaler.n_features_in_:\n",
    "                        print(f\"Skipped frame (got {vec.shape[0]} features, expected {scaler.n_features_in_}).\")\n",
    "                    else:\n",
    "                        vec_scaled = scaler.transform(vec.reshape(1, -1))\n",
    "                        pred_idx = knn.predict(vec_scaled)[0]\n",
    "                        if hasattr(knn, \"predict_proba\"):\n",
    "                            probs = knn.predict_proba(vec_scaled)[0]\n",
    "                            conf = probs[pred_idx]\n",
    "                            prob_text = f\"{conf*100:.1f}%\"\n",
    "                        label_text = idx2label[int(pred_idx)]\n",
    "\n",
    "            cv2.putText(img, f\"Gesture: {label_text}\", (10,30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "            if prob_text:\n",
    "                cv2.putText(img, f\"Conf: {prob_text}\", (10,65),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,200,255), 2)\n",
    "            cv2.imshow(\"Hand Gesture Recognition\", img)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COLLECT DATA MODE ===\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "DATASET_PATH = \"landmarks.npy\"\n",
    "\n",
    "# Lable list\n",
    "labels = ['A', 'B', 'C', 'D', 'E', 'G', 'H', 'I', 'K', 'L', 'M',\n",
    "          'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y',\n",
    "          'Ă', 'Â', 'Ê', 'Ô', 'Ơ', 'Ư', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "def get_sample_count(label):\n",
    "    #Count sample of each label.\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        return 0\n",
    "    data = np.load(DATASET_PATH, allow_pickle=True)\n",
    "    return sum(1 for item in data if item[0] == label)\n",
    "\n",
    "def make_options():\n",
    "    #Count samples of each label in the list.\n",
    "    return [(f\"{label} ({get_sample_count(label)} samples)\", label) for label in labels]\n",
    "\n",
    "# drop-down with labels + num of samples\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=make_options(),\n",
    "    value=labels[0],\n",
    "    description='Choose label:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Collect button\n",
    "button = widgets.Button(description=\"Collect\")\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    print(f\"Collect for label: {dropdown.value}\")\n",
    "    collect(dropdown.value)  \n",
    "    print(f\"Finish collecting label: {dropdown.value}\")\n",
    "\n",
    "    # Update num of samples\n",
    "    dropdown.options = make_options()\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "display(dropdown, button)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN MODE ===\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREDICT MODE ===\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === XEM DANH SÁCH LABEL TRONG DỮ LIỆU landmarks.npy ===\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "DATASET_PATH = \"landmarks.npy\"\n",
    "\n",
    "try:\n",
    "    data = np.load(DATASET_PATH, allow_pickle=True)\n",
    "    print(f\"Đã tải dữ liệu từ {DATASET_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Không tìm thấy file {DATASET_PATH}\")\n",
    "else:\n",
    "    # Mỗi phần tử có dạng (label, vector)\n",
    "    labels = [item[0] for item in data]\n",
    "\n",
    "    # Đếm số lượng mẫu cho từng label\n",
    "    label_counts = Counter(labels)\n",
    "\n",
    "    print(\"\\nDanh sách các label đã thu thập:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"  - {label:<15}: {count} mẫu\")\n",
    "\n",
    "    print(f\"\\nTổng số mẫu: {len(labels)}\")\n",
    "    print(f\"Số lượng label khác nhau: {len(label_counts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phân tích kết quả, vấn đề gặp phải\n",
    "1. Phân tích kết quả\n",
    " - Nhóm đã tạo ra mô hình Trí tuệ nhân tạo có khả năng nhận diện các ngón tay và cử chỉ, có độ chính xác cao.\n",
    " - Mô hình Trí tuệ nhân tạo sử dụng mô hình huấn luyện K-Nearest Neighbors (KNN), sau quá trình huấn luyện đã có khả năng phân biệt các cử chỉ tay cơ bản, với độ chính xác khá cao khi các cử chỉ có sự khác nhau nhiều.\n",
    " - Cho đến hiện tại, mô hình vẫn đang hoạt động khá tốt, tuy nhiên thi thoảng có thể gặp vài lỗi nhỏ khi đang chạy, chủ yếu do mặt logic có thể còn chưa đủ, chưa xét hết các trường hợp.\n",
    "2. Vấn đề gặp phải\n",
    " - Các thành viên nhóm đều chưa có kinh nghiệm trong việc làm các project về AI trước đây, nên lúc thực hiện mở rộng project vẫn còn nhiều khó khăn, vướng mắc. Tuy nhiên, nhờ sự phổ biến của các kênh thông tin, việc tiếp cận cách xây dựng project trở nên thuận tiện hơn. Tuy nhiên, cho đến hiện tại project vẫn đang cần sửa chữa, hoàn thiện thêm.\n",
    " - Các thành viên chưa thống nhất được việc ứng dụng mô hình trên vào vấn đề thực tế nào, hiện nay, nhóm đang có 2 hướng: Áp dụng vào việc nhận biết cử chỉ tay để chuyển thành chữ tương ứng (theo chuẩn ASL hoặc VSL), hoặc Ứng dụng vào việc sử dụng cử chỉ tay để điều khiển máy tính ở khoảng cách vừa (2-3 m)\n",
    " - Về tiến độ thực hiện, nhóm sẽ cố gắng thống nhất việc ứng dụng mô hình vào đâu trong tuần tới, để nhanh chóng hoàn thiện các bước tiếp theo như đánh giá, kiểm thử, đưa ra bản chính thức của project để viết báo cáo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cập nhật kết quả cuối kỳ (W15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi tiết phương pháp, dữ liệu \n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chương trình\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phân tích, đánh giá kết quả\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cập nhật phân công, khối lượng công việc\n",
    "<!-- công việc của các thành viên, tỷ lệ đóng góp của các thành viên -->\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
